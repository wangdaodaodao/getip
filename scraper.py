# scraper.py
import requests
import re
import os
import datetime
import yaml
import json
import base64
from urllib.parse import quote

# --- æ ¸å¿ƒé…ç½®åŒº ---
BASE_ID = 196
BASE_DATE_STR = "2025-09-19"

def calculate_current_url():
    """æ ¹æ®å½“å‰æ—¥æœŸè®¡ç®—å‡ºå½“å¤©çš„ç›®æ ‡URLã€‚"""
    print("æ­¥éª¤ 1: æ­£åœ¨æ ¹æ®å½“å‰æ—¥æœŸè®¡ç®—ç›®æ ‡URL...")
    try:
        base_date = datetime.datetime.strptime(BASE_DATE_STR, "%Y-%m-%d").date()
        today = datetime.datetime.utcnow().date()
        delta_days = (today - base_date).days
        current_id = BASE_ID + delta_days
        target_url = f"https://nodesdz.com/?id={current_id}"
        print(f"ç”Ÿæˆçš„ä»Šæ—¥URL: {target_url}")
        return target_url
    except Exception as e:
        print(f"é”™è¯¯ï¼šè®¡ç®—URLæ—¶å‘ç”Ÿé”™è¯¯ - {e}")
        return None

def create_vless_link_from_clash(proxy):
    """ä»ŽClashçš„proxyå­—å…¸ç›´æŽ¥åˆ›å»ºVLESSé“¾æŽ¥"""
    if proxy.get("type") != "vless": return None
    name = quote(proxy.get("name", "Unnamed"))
    params = {
        "security": "reality", "sni": proxy.get("servername", ""),
        "fp": proxy.get("client-fingerprint", "chrome"),
        "publicKey": proxy.get("reality-opts", {}).get("public-key", ""),
        "shortId": proxy.get("reality-opts", {}).get("short-id", ""),
        "flow": proxy.get("flow", "")
    }
    param_str = '&'.join([f"{k}={v}" for k, v in params.items() if v])
    return f"vless://{proxy.get('uuid')}@{proxy.get('server')}:{proxy.get('port')}?{param_str}#{name}"

def main():
    target_url = calculate_current_url()
    if not target_url: return

    try:
        print(f"æ­¥éª¤ 2 & 3: æ­£åœ¨æŠ“å–å’Œæå–Clashè®¢é˜…é“¾æŽ¥...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        page_res = requests.get(target_url, headers=headers, timeout=15)
        page_res.raise_for_status()
        match = re.search(r'clash\s*:\s*"(https?://[^\s"]+)"', page_res.text)
        if not match: raise ValueError("åœ¨é¡µé¢ä¸­æœªæ‰¾åˆ°Clashè®¢é˜…é“¾æŽ¥")
        sub_link = match.group(1)
        print(f"æå–æˆåŠŸ -> {sub_link}")

        print("æ­¥éª¤ 4: æ­£åœ¨ä¸‹è½½YAMLè®¢é˜…å†…å®¹...")
        sub_headers = {'User-Agent': 'ClashforWindows/0.20.19'}
        sub_res = requests.get(sub_link, headers=sub_headers, timeout=15)
        sub_res.raise_for_status()
        
        print("æ­¥éª¤ 5: æ­£åœ¨è§£æžYAML...")
        data = yaml.safe_load(sub_res.text)
        proxies = data.get('proxies', [])
        if not proxies: raise ValueError("YAMLæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°proxies")
        
        print(f"è§£æžåˆ° {len(proxies)} ä¸ªèŠ‚ç‚¹ã€‚")
        
        # =================================================================== #
        # =================== åœ¨è¿™é‡Œè¿›è¡Œè‡ªå®šä¹‰åç§°ä¿®æ”¹ ====================== #
        # =================================================================== #
        
        print("æ­£åœ¨è¿›è¡Œæ™ºèƒ½é‡å‘½åå’Œåœ°åŒºè¯†åˆ«...")
        for proxy in proxies:
            name = proxy.get("name", "")
            
            if "é¦™æ¸¯" in name:
                proxy["name"] = f"ðŸ‡­ðŸ‡° é¦™æ¸¯"
            elif "æ—¥æœ¬" in name:
                proxy["name"] = f"ðŸ‡¯ðŸ‡µ æ—¥æœ¬"
            elif "æ–°åŠ å¡" in name:
                proxy["name"] = f"ðŸ‡¸ðŸ‡¬ æ–°åŠ å¡"
            elif "ç¾Žå›½" in name:
                proxy["name"] = f"ðŸ‡ºðŸ‡¸ ç¾Žå›½"
            # å¦‚æžœä¸Šé¢å…³é”®è¯éƒ½ä¸å­˜åœ¨ï¼Œåˆ™æ‰§è¡Œæ‚¨çš„è¦æ±‚
            else:
                proxy["name"] = f"ðŸ‡¨ðŸ‡³ ä¸­å›½"

        print("æ­£åœ¨ä¸ºé‡åèŠ‚ç‚¹æ·»åŠ åºå·...")
        name_counts = {}
        processed_proxies = []
        for proxy in proxies:
            name = proxy.get("name")
            current_count = name_counts.get(name, 0) + 1
            name_counts[name] = current_count
            # åªä¸ºç¬¬äºŒä¸ªåŠä»¥åŽçš„åŒåèŠ‚ç‚¹æ·»åŠ åºå·
            if current_count > 1:
                proxy["name"] = f"{name}-{current_count}"
            processed_proxies.append(proxy)
        proxies = processed_proxies # ä½¿ç”¨å¤„ç†åŽçš„æ–°åˆ—è¡¨

        # =================================================================== #
        # ========================= ä¿®æ”¹ç»“æŸ ================================ #
        # =================================================================== #

        output_dir = 'public'
        if not os.path.exists(output_dir): os.makedirs(output_dir)

        print("æ­£åœ¨ç”Ÿæˆ nodes.json ...")
        json_path = os.path.join(output_dir, 'nodes.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(proxies, f, indent=2, ensure_ascii=False)
        print(f"å·²å°†å¤„ç†åŽçš„èŠ‚ç‚¹ä¿¡æ¯ä¿å­˜åˆ° -> {json_path}")

        print("æ­£åœ¨ç”Ÿæˆé€šç”¨ Base64 è®¢é˜…æ–‡ä»¶ sub.txt ...")
        vless_links = [link for proxy in proxies if (link := create_vless_link_from_clash(proxy))]
        
        subscription_text = "\n".join(vless_links)
        encoded_subscription = base64.b64encode(subscription_text.encode('utf-8')).decode('utf-8')
        
        sub_path = os.path.join(output_dir, 'sub.txt')
        with open(sub_path, 'w', encoding='utf-8') as f:
            f.write(encoded_subscription)
        print(f"å·²å°†é€šç”¨è®¢é˜…æ–‡ä»¶ä¿å­˜åˆ° -> {sub_path}")

    except Exception as e:
        print(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()